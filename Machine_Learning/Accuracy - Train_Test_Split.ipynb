{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "#import da base\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declarando funções que serão usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declarando o conteúdo da atividade em funções (KNN, decision tree e random forest)\n",
    "def knn_bagging(Xtrain,ytrain,Xtest):\n",
    "    #dados sem reducao\n",
    "    start = time.time()\n",
    "    model = KNeighborsClassifier()\n",
    "    scores = cross_val_score(model, Xtrain, ytrain, cv=5)\n",
    "    print('Acurácia de KNeighbors simples:', scores.mean())\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    y_pred_1 = model.predict(Xtest)\n",
    "    \n",
    "    start = time.time()\n",
    "    model = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5, random_state = 42)\n",
    "    scores = cross_val_score(model, Xtrain, ytrain, cv=5)\n",
    "    print('Acurácia de KNeighbors Bagging (c/ 10 estimators):', scores.mean())\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    y_pred_2 = model.predict(Xtest)\n",
    "    \n",
    "    start = time.time()\n",
    "    model = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5, n_estimators=100, random_state = 42)\n",
    "    scores = cross_val_score(model, Xtrain, ytrain, cv=5)\n",
    "    print('Acurácia de KNeighbors Bagging (c/ 100 estimators):', scores.mean())\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    y_pred_3 = model.predict(Xtest)\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Teste do McNemar')\n",
    "    testaMcNemar(ytrain,y_pred_1,y_pred_2)\n",
    "    testaMcNemar(ytrain,y_pred_1,y_pred_3)\n",
    "    testaMcNemar(ytrain,y_pred_2,y_pred_3)\n",
    "    \n",
    "    return y_pred_1,y_pred_2,y_pred_3\n",
    "#################################################################################################\n",
    "    \n",
    "def decision_tree(Xtrain,ytrain,Xtest):\n",
    "    start = time.time()\n",
    "    model = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "    scores = cross_val_score(model, Xtrain, ytrain, cv=5)\n",
    "    print('Acurácia de Decision Tree puro:', scores.mean())\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    y_pred_1 = model.predict(Xtest)\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    model = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    scores = cross_val_score(model, Xtrain, ytrain, cv=5)\n",
    "    print('Acurácia de Random Forest:', scores.mean())\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    y_pred_2 = model.predict(Xtest)\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    model = ExtraTreesClassifier(n_estimators=50, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    scores = cross_val_score(model, Xtrain, ytrain, cv=5)\n",
    "    print('Acurácia de Extreme Randomized Trees:', scores.mean())\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    y_pred_3 = model.predict(Xtest)\n",
    "\n",
    "    print('-------------------------------------')\n",
    "    print('Teste do McNemar')\n",
    "    testaMcNemar(ytrain,y_pred_1,y_pred_2)\n",
    "    testaMcNemar(ytrain,y_pred_1,y_pred_3)\n",
    "    testaMcNemar(ytrain,y_pred_2,y_pred_3)\n",
    "    \n",
    "    return y_pred_1,y_pred_2,y_pred_3\n",
    "########################################################################################################\n",
    "\n",
    "def multiples_vote(Xtrain,ytrain,Xtest):\n",
    "    start = time.time()\n",
    "    #Obs aumentei a max_inter da regressão logistica para tirar os warnings (deixou bem lento, tem a ver com a escala dos dados)\n",
    "    clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',max_iter = 10000, random_state=1)\n",
    "    clf2 = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "    clf3 = GaussianNB()\n",
    "\n",
    "    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb',clf3)], voting='hard')\n",
    "\n",
    "    for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "        print(\"Acurácia: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n",
    "    print('-'*20)\n",
    "\n",
    "    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='hard')\n",
    "\n",
    "    for clf, label in zip([clf1, clf2, eclf], ['Logistic Regression', 'Random Forest', 'Ensemble']):\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "        print(\"Acurácia: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "def gradient_boost(Xtrain,ytrain,Xtest):\n",
    "    \n",
    "    start = time.time()\n",
    "    model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.3, max_depth=2, random_state=0)\n",
    "    scores = cross_val_score(model, Xtrain, ytrain, cv=5)\n",
    "\n",
    "    print('Acurácia de Gradient Boosting Tree:', scores.mean())\n",
    "    end = time.time()\n",
    "    print('Tempo:',end - start)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    y_pred_1 = model.predict(Xtest)\n",
    "\n",
    "    return y_pred_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazer a parte do mc nemmar: A ideia é fazer 1 comparação com cada tipo (originais,redução e seleção):\n",
    "\n",
    "#essa primeira função cria a contigence table a partir do y_test,y_pred1 e y_pred2 (resultados que vc quer comparar)\n",
    "def build_contingence_table(Y, Y_pred_1, Y_pred_2):\n",
    "    y1_and_y2 = 0\n",
    "    y1_and_not_y2 = 0\n",
    "    y2_and_not_y1 = 0\n",
    "    not_y1_and_not_y2 = 0\n",
    "    for y, y1, y2 in zip(Y, Y_pred_1, Y_pred_2):\n",
    "        if y == y1 == y2:\n",
    "            y1_and_y2 += 1\n",
    "        elif y != y1 and y != y2:\n",
    "            not_y1_and_not_y2 += 1\n",
    "        elif y == y1 and y != y2:\n",
    "            y1_and_not_y2 += 1\n",
    "        elif y != y1 and y == y2:\n",
    "            y2_and_not_y1 += 1\n",
    "            \n",
    "    contingency_table = [[y1_and_y2, y1_and_not_y2], \n",
    "                         [y2_and_not_y1, not_y1_and_not_y2]]\n",
    "    \n",
    "    return contingency_table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testaMcNemar(train, pred1, pred2):\n",
    "    contingence_table = build_contingence_table(train, pred1, pred2)\n",
    "\n",
    "    import pprint\n",
    "\n",
    "    pprint.pprint(contingence_table)\n",
    "\n",
    "    result = mcnemar(contingence_table, exact=True)\n",
    "\n",
    "\n",
    "    if result.pvalue >= 0.001:\n",
    "        print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))\n",
    "    else:\n",
    "        print('statistic=%.3f, p-value=%.3e' % (result.statistic, result.pvalue))\n",
    "\n",
    "    # interpretando o p-value\n",
    "    alpha = 0.05\n",
    "    if result.pvalue > alpha:\n",
    "        print('Mesma proporção de erros (falhou em rejeitar H0)')\n",
    "    else:\n",
    "        print('Proporções de erros diferentes (rejeitou H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividindo em treino e teste para cada tipo (original, redução e seleção)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número original de atributos: 64\n",
      "Número reduzido de atributos: 21\n",
      "Número de atributos de Seleçao: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:114: UserWarning: Features [ 0 32 39] are constant.\n",
      "  UserWarning)\n",
      "D:\\python\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "D:\\python\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:114: UserWarning: Features [ 0 16 24 32 39 56] are constant.\n",
      "  UserWarning)\n",
      "D:\\python\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "# Aplicando Train test split nos dados originais\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print('Número original de atributos:', X.shape[1])\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Aplicando Train test split nos dados com reduçao de atributos (PCA)\n",
    "\n",
    "#realizando a redução\n",
    "pca = PCA(n_components=0.90, whiten=True)\n",
    "\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print('Número reduzido de atributos:', X_pca.shape[1])\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Aplicando Train test split na seleção de atributos\n",
    "X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "fvalue_selector = SelectKBest(f_classif, k=20)\n",
    "X_kbest = fvalue_selector.fit_transform(X_train_sel, y_train_sel)\n",
    "X_kbest2= fvalue_selector.fit_transform(X_test_sel, y_test_sel)\n",
    "print('Número de atributos de Seleçao:', X_kbest.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocando tudo dentro de um dicionário e rodando com um loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'originais':[X_train,y_train,X_test],'redução':[X_train_pca,y_train_pca,X_test_pca],'seleção':[X_kbest,y_train_sel,X_kbest2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "originais\n",
      "redução\n",
      "seleção\n"
     ]
    }
   ],
   "source": [
    "#essa parte é só um teste\n",
    "# observação d[k][0] é X e d[k][1] é y\n",
    "for k in d.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================\n",
      "Tempos e acurácias dos modelos com os dados originais :\n",
      "==========================================================================\n",
      "Acurácia de KNeighbors simples: 0.9767150760719225\n",
      "Tempo: 0.226454496383667\n",
      "Acurácia de KNeighbors Bagging (c/ 10 estimators): 0.9709163208852004\n",
      "Tempo: 0.5030057430267334\n",
      "Acurácia de KNeighbors Bagging (c/ 100 estimators): 0.9717461964038726\n",
      "Tempo: 4.986911296844482\n",
      "-------------------------------------\n",
      "Teste do McNemar\n",
      "[[53, 2], [3, 536]]\n",
      "statistic=2.000, p-value=1.000\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[55, 0], [1, 538]]\n",
      "statistic=0.000, p-value=1.000\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[54, 2], [2, 536]]\n",
      "statistic=2.000, p-value=1.000\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "_____________________\n",
      "Acurácia de Decision Tree puro: 0.8204426002766253\n",
      "Tempo: 0.04686737060546875\n",
      "Acurácia de Random Forest: 0.9592842323651452\n",
      "Tempo: 0.5819847583770752\n",
      "Acurácia de Extreme Randomized Trees: 0.971753112033195\n",
      "Tempo: 0.4498403072357178\n",
      "-------------------------------------\n",
      "Teste do McNemar\n",
      "[[49, 6], [8, 531]]\n",
      "statistic=6.000, p-value=0.791\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[48, 7], [9, 530]]\n",
      "statistic=7.000, p-value=0.804\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[56, 1], [1, 536]]\n",
      "statistic=1.000, p-value=1.000\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "_____________________\n",
      "Acurácia: 0.95 (+/- 0.01) [Logistic Regression]\n",
      "Acurácia: 0.93 (+/- 0.01) [Random Forest]\n",
      "Acurácia: 0.81 (+/- 0.05) [naive Bayes]\n",
      "Acurácia: 0.94 (+/- 0.01) [Ensemble]\n",
      "--------------------\n",
      "Acurácia: 0.95 (+/- 0.01) [Logistic Regression]\n",
      "Acurácia: 0.93 (+/- 0.01) [Random Forest]\n",
      "Acurácia: 0.94 (+/- 0.01) [Ensemble]\n",
      "Tempo: 30.958686590194702\n",
      "_____________________\n",
      "Acurácia de Gradient Boosting Tree: 0.9576106500691562\n",
      "Tempo: 10.685279607772827\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "Tempos e acurácias dos modelos com os dados redução :\n",
      "==========================================================================\n",
      "Acurácia de KNeighbors simples: 0.9692392807745505\n",
      "Tempo: 0.09372186660766602\n",
      "Acurácia de KNeighbors Bagging (c/ 10 estimators): 0.9625933609958507\n",
      "Tempo: 0.2327406406402588\n",
      "Acurácia de KNeighbors Bagging (c/ 100 estimators): 0.9734094052558783\n",
      "Tempo: 2.2966601848602295\n",
      "-------------------------------------\n",
      "Teste do McNemar\n",
      "[[56, 0], [3, 535]]\n",
      "statistic=0.000, p-value=0.250\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[55, 1], [2, 536]]\n",
      "statistic=1.000, p-value=1.000\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[56, 3], [1, 534]]\n",
      "statistic=1.000, p-value=0.625\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "_____________________\n",
      "Acurácia de Decision Tree puro: 0.8462102351313969\n",
      "Tempo: 0.07810711860656738\n",
      "Acurácia de Random Forest: 0.9534405255878285\n",
      "Tempo: 0.7642016410827637\n",
      "Acurácia de Extreme Randomized Trees: 0.9733955739972338\n",
      "Tempo: 0.42352795600891113\n",
      "-------------------------------------\n",
      "Teste do McNemar\n",
      "[[48, 8], [8, 530]]\n",
      "statistic=8.000, p-value=1.000\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[46, 10], [11, 527]]\n",
      "statistic=10.000, p-value=1.000\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[54, 2], [3, 535]]\n",
      "statistic=2.000, p-value=1.000\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "_____________________\n",
      "Acurácia: 0.95 (+/- 0.01) [Logistic Regression]\n",
      "Acurácia: 0.93 (+/- 0.01) [Random Forest]\n",
      "Acurácia: 0.81 (+/- 0.05) [naive Bayes]\n",
      "Acurácia: 0.94 (+/- 0.01) [Ensemble]\n",
      "--------------------\n",
      "Acurácia: 0.95 (+/- 0.01) [Logistic Regression]\n",
      "Acurácia: 0.93 (+/- 0.01) [Random Forest]\n",
      "Acurácia: 0.94 (+/- 0.01) [Ensemble]\n",
      "Tempo: 31.73463225364685\n",
      "_____________________\n",
      "Acurácia de Gradient Boosting Tree: 0.9443222683264176\n",
      "Tempo: 17.285876512527466\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "Tempos e acurácias dos modelos com os dados seleção :\n",
      "==========================================================================\n",
      "Acurácia de KNeighbors simples: 0.9468084370677732\n",
      "Tempo: 0.09471774101257324\n",
      "Acurácia de KNeighbors Bagging (c/ 10 estimators): 0.9193845089903181\n",
      "Tempo: 0.22539782524108887\n",
      "Acurácia de KNeighbors Bagging (c/ 100 estimators): 0.9384958506224066\n",
      "Tempo: 2.2924513816833496\n",
      "-------------------------------------\n",
      "Teste do McNemar\n",
      "[[36, 16], [23, 519]]\n",
      "statistic=16.000, p-value=0.337\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[44, 8], [14, 528]]\n",
      "statistic=8.000, p-value=0.286\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[41, 18], [17, 518]]\n",
      "statistic=17.000, p-value=1.000\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "_____________________\n",
      "Acurácia de Decision Tree puro: 0.8179598893499309\n",
      "Tempo: 0.033911943435668945\n",
      "Acurácia de Random Forest: 0.9360235131396957\n",
      "Tempo: 0.6193199157714844\n",
      "Acurácia de Extreme Randomized Trees: 0.9459716459197788\n",
      "Tempo: 0.4617640972137451\n",
      "-------------------------------------\n",
      "Teste do McNemar\n",
      "[[17, 41], [38, 498]]\n",
      "statistic=38.000, p-value=0.822\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[13, 45], [42, 494]]\n",
      "statistic=42.000, p-value=0.830\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "[[37, 18], [18, 521]]\n",
      "statistic=18.000, p-value=1.000\n",
      "Mesma proporção de erros (falhou em rejeitar H0)\n",
      "_____________________\n",
      "Acurácia: 0.95 (+/- 0.01) [Logistic Regression]\n",
      "Acurácia: 0.93 (+/- 0.01) [Random Forest]\n",
      "Acurácia: 0.81 (+/- 0.05) [naive Bayes]\n",
      "Acurácia: 0.94 (+/- 0.01) [Ensemble]\n",
      "--------------------\n",
      "Acurácia: 0.95 (+/- 0.01) [Logistic Regression]\n",
      "Acurácia: 0.93 (+/- 0.01) [Random Forest]\n",
      "Acurácia: 0.94 (+/- 0.01) [Ensemble]\n",
      "Tempo: 36.40570545196533\n",
      "_____________________\n",
      "Acurácia de Gradient Boosting Tree: 0.9368222683264177\n",
      "Tempo: 8.1324942111969\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#o d2 é mais um dicionário para armazenar todos os y_pred de cada modelo (ainda tenho que organiza-lo melhor)\n",
    "\n",
    "#OBS: FALTA FAZER A FUNÇÃO DO MULTIPLE VOTES JOGAR O Y_PRED NA VARIÁVEL (AINDA NÃO CONSEGUI)\n",
    "\n",
    "d2={}\n",
    "for k in d.keys():\n",
    "    print('==========================================================================')\n",
    "    print('Tempos e acurácias dos modelos com os dados',k,':')\n",
    "    print('==========================================================================')\n",
    "    y_pred_b1,y_pred_b2,y_pred_b3 = knn_bagging(d[k][0],d[k][1],d[k][2])  \n",
    "    d2[k + '_bagging'] = y_pred_b1,y_pred_b2,y_pred_b3\n",
    "    print('_____________________')\n",
    "    y_pred_t1,y_pred_t2,y_pred_t3 = decision_tree(d[k][0],d[k][1],d[k][2])\n",
    "    d2[k + '_tree'] = y_pred_t1,y_pred_t2,y_pred_t3\n",
    "    print('_____________________')\n",
    "    multiples_vote(d[k][0],d[k][1],d[k][2])\n",
    "    print('_____________________')\n",
    "    y_pred_g1 = gradient_boost(d[k][0],d[k][1],d[k][2])\n",
    "    d2[k + '_boost'] = y_pred_g1\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
